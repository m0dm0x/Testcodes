from pyspark.sql.utils import AnalysisException
from edh_ingestion_modules.models.task import Task
from edh_ingestion_modules.models.task_status import TaskStatus
from edh_ingestion_modules.utilities.exceptions import TransformationFailedError


class TransformationTask(Task):
    def __init__(self, task_config: dict):
        """
        Initializes the TransformationTask with the task configuration.

        Args:
            task_config (dict): Configuration containing task-specific parameters.
        """
        self.task_id = task_config.get("taskId")
        self.input_df_id = task_config.get("input_df_id")
        self.output_df_id = task_config.get("output_df_id")
        self.sql_file = task_config.get("sql_file")
        self.delimiter = task_config.get("delimiter", ",")
        self.continue_on_missing = task_config.get("continueOnMissing", False)

    def execute(self):
        """
        Executes the transformation task:
        - Reads the input DataFrame from the snapshot.
        - Applies the transformation SQL query.
        - Saves the transformed DataFrame back to the snapshot.

        Returns:
            tuple: TaskStatus and error (if any).
        """
        # Get Spark session, logger, and snapshots
        spark = self.spark_instance
        logger = self.logger_instance
        snapshots = self.snapshot_instance

        try:
            # Check for required configurations
            if not self.input_df_id or not self.output_df_id or not self.sql_file:
                raise ValueError("Required configurations 'input_df_id', 'output_df_id', or 'sql_file' are missing.")

            logger.info(f"Starting TransformationTask with ID: {self.task_id}")
            logger.info(f"Fetching input DataFrame '{self.input_df_id}' from snapshot...")
            input_df = snapshots.get_snapshot(self.input_df_id)

            # Handle missing DataFrame based on the `continueOnMissing` flag
            if not input_df:
                if self.continue_on_missing:
                    logger.warning(f"Input DataFrame '{self.input_df_id}' is missing. Skipping transformation.")
                    return TaskStatus.SUCCESS, None
                else:
                    raise ValueError(f"Input DataFrame '{self.input_df_id}' not found in snapshot.")

            # Load SQL query from the file
            logger.info(f"Loading SQL query from: {self.sql_file}")
            with open(self.sql_file, "r") as f:
                sql_query = f.read()

            # Repartition for optimization (optional)
            input_df = input_df.repartition(200)

            # Create a temporary view for SQL processing
            logger.info("Creating temporary view for SQL transformation...")
            input_df.createOrReplaceTempView("input_table")

            # Execute the SQL transformation
            logger.info("Executing SQL transformation...")
            transformed_df = spark.sql(sql_query)

            # Persist the transformed DataFrame
            transformed_df = transformed_df.persist()

            # Save the transformed DataFrame to the snapshot
            logger.info(f"Saving transformed DataFrame to snapshot with ID: {self.output_df_id}")
            snapshots.add_snapshot(self.output_df_id, transformed_df)

            logger.info(f"TransformationTask with ID '{self.task_id}' completed successfully.")
            return TaskStatus.SUCCESS, None

        except AnalysisException as exc:
            logger.warning(f"SQL execution failed in task '{self.task_id}': {exc}")
            raise TransformationFailedError(
                f"SQL transformation failed in task '{self.task_id}' due to analysis exception: {exc}"
            ) from exc
        except Exception as exc:
            logger.warning(f"An error occurred in TransformationTask '{self.task_id}': {exc}")
            raise TransformationFailedError(
                f"TransformationTask '{self.task_id}' encountered an error: {exc}"
            ) from exc