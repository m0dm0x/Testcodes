from pyspark.sql import SparkSession
from pyspark.sql.functions import broadcast
from pyspark.storagelevel import StorageLevel
from pyspark.errors import PySparkRuntimeError
from edh_ingestion_modules.models.task import Task
from edh_ingestion_modules.models.task_status import TaskStatus
from edh_ingestion_modules.utilities.exceptions import TransformationFailedError


class TransformationTask(Task):
    """
    A task to transform data using SQL queries, optimized with Spark plans and persistence.

    Configuration:
    - taskId: TransformationTask
    - input_df_id: ID of the input DataFrame saved in the snapshot.
    - sql_file: Path to the SQL file containing transformation logic.
    - output_df_id: ID to save the transformed DataFrame.
    - continueOnMissing: Flag to determine whether to proceed if the input DataFrame is missing (default: False).
    """

    def __init__(self, task_config: dict):
        self.task_config = task_config
        self.task_id = task_config.get("taskId")

    def execute(self):
        spark = self.spark_instance
        logger = self.logger_instance
        snapshots = self.snapshot_instance

        try:
            # Extract configurations
            input_df_id = self.task_config.get("input_df_id")
            output_df_id = self.task_config.get("output_df_id")
            sql_file = self.task_config.get("sql_file")
            continue_on_missing = self.task_config.get("continueOnMissing", False)

            logger.info(f"Starting TransformationTask with taskId: {self.task_id}")
            logger.info(f"Input DataFrame: {input_df_id}")
            logger.info(f"SQL File Path: {sql_file}")
            logger.info(f"Output DataFrame: {output_df_id}")
            logger.info(f"Continue On Missing: {continue_on_missing}")

            # Check if the input DataFrame exists in the snapshot
            input_df = snapshots.get_snapshot(input_df_id)
            if not input_df:
                logger.warning(f"Input DataFrame '{input_df_id}' is missing.")
                if not continue_on_missing:
                    raise TransformationFailedError(
                        f"Required DataFrame '{input_df_id}' is missing and continueOnMissing is False."
                    )
                else:
                    logger.info("Continuing without executing transformation as continueOnMissing is True.")
                    return TaskStatus.SUCCESS, None

            # Analyze DataFrame size and apply optimizations
            input_df_count = input_df.count()
            logger.info(f"Input DataFrame '{input_df_id}' has {input_df_count} records.")

            if input_df_count < 1_000_000:
                logger.info("Applying broadcast hint and caching for small DataFrame.")
                input_df = broadcast(input_df).persist(StorageLevel.MEMORY_AND_DISK)
            elif input_df_count > 1_000_000_000:
                logger.info("Applying coalesce for large DataFrame.")
                input_df = input_df.coalesce(spark.sparkContext.defaultParallelism * 2)
            else:
                logger.info("Repartitioning for medium-sized DataFrame.")
                input_df = input_df.repartition(spark.sparkContext.defaultParallelism * 4)

            # Show the Spark plans for debugging and optimization
            logger.info("Logical and Physical Plans for the Input DataFrame:")
            input_df.explain(extended=True)

            # Create a temporary view for the input DataFrame
            input_df.createOrReplaceTempView(input_df_id)
            logger.info(f"Temporary view created for DataFrame: {input_df_id}")

            # Read SQL file
            with open(sql_file, "r") as file:
                sql_query = file.read()

            logger.info("Executing SQL Query...")
            transformed_df = spark.sql(sql_query)

            # Show the Spark plans for the transformed DataFrame
            logger.info("Logical and Physical Plans for the Transformed DataFrame:")
            transformed_df.explain(extended=True)

            # Persist the transformed DataFrame
            transformed_df.persist(StorageLevel.MEMORY_AND_DISK)
            transformed_df.count()  # Trigger an action to persist the DataFrame
            logger.info("Transformed DataFrame persisted successfully.")

            # Add the transformed DataFrame to snapshots
            snapshots.add_snapshot(output_df_id, transformed_df)
            logger.info(f"Transformation completed successfully. Output saved as: {output_df_id}")

            return TaskStatus.SUCCESS, None

        except PySparkRuntimeError as e:
            logger.error(f"PySpark Runtime Error: {str(e)}")
            return TaskStatus.FAILED, e

        except TransformationFailedError as e:
            logger.error(f"Transformation Failed Error: {str(e)}")
            return TaskStatus.FAILED, e

        except Exception as e:
            logger.error(f"Unexpected Error: {str(e)}")
            return TaskStatus.FAILED, e