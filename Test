from pyspark.sql import SparkSession
from pyspark.errors import PySparkRuntimeError
from edh_ingestion_modules.models.task import Task
from edh_ingestion_modules.models.task_status import TaskStatus
from edh_ingestion_modules.utilities.exceptions import TransformationFailedError


class TransformationTask(Task):
    """
    A task to transform data using SQL queries. It supports handling multiple input DataFrames,
    dynamic SQL files, and optional continuation on missing DataFrames.

    Configuration:
    - taskId: TransformationTask
    - input_df_id: Comma-separated IDs of input DataFrames saved in the snapshot.
    - output_df_id: ID to save the transformed DataFrame.
    - sql_file: Path to the SQL file containing transformation logic.
    - delimiter: Delimiter for separating multiple input DataFrame IDs (default: ',').
    - continueOnMissing: Flag to determine whether to proceed if some input DataFrames are missing (default: False).
    """

    def __init__(self, task_config: dict):
        """
        Initializes the TransformationTask with the provided configuration.
        """
        self.task_config = task_config
        self.task_id = task_config.get("taskId")

    def execute(self):
        """
        Executes the transformation task.

        Returns:
            - TaskStatus.SUCCESS if the transformation is successful.
            - TaskStatus.FAILED if an error occurs.
        """
        spark = self.spark_instance
        logger = self.logger_instance
        snapshots = self.snapshot_instance

        try:
            # Extract configurations
            input_df_ids = self.task_config.get("input_df_id").split(
                self.task_config.get("delimiter", ",")
            )
            output_df_id = self.task_config.get("output_df_id")
            sql_file = self.task_config.get("sql_file")
            continue_on_missing = self.task_config.get("continueOnMissing", False)

            logger.info("Starting TransformationTask...")
            logger.info(f"Input DataFrames: {input_df_ids}")
            logger.info(f"SQL File Path: {sql_file}")
            logger.info(f"Continue On Missing: {continue_on_missing}")

            # Check existence of input DataFrames
            available_dfs = [
                snapshots.get_snapshot(df_id) for df_id in input_df_ids if snapshots.get_snapshot(df_id)
            ]
            missing_dfs = [df_id for df_id in input_df_ids if not snapshots.get_snapshot(df_id)]

            if missing_dfs:
                logger.warning(f"Missing DataFrames: {missing_dfs}")
                if not continue_on_missing:
                    raise TransformationFailedError(
                        f"Required DataFrames {missing_dfs} are missing and continueOnMissing is False."
                    )

            # Create temporary views for available DataFrames
            for df_id, df in zip(input_df_ids, available_dfs):
                if df is not None:
                    df.createOrReplaceTempView(df_id)
                    logger.info(f"Temporary view created for DataFrame: {df_id}")

            # Read SQL file
            with open(sql_file, "r") as file:
                sql_query = file.read()

            logger.info("Executing SQL Query...")
            transformed_df = spark.sql(sql_query)

            # Add the transformed DataFrame to snapshots
            snapshots.add_snapshot(output_df_id, transformed_df)
            logger.info(f"Transformation completed successfully. Output saved as: {output_df_id}")

            return TaskStatus.SUCCESS, None

        except PySparkRuntimeError as e:
            logger.error(f"PySpark Runtime Error: {str(e)}")
            return TaskStatus.FAILED, e

        except TransformationFailedError as e:
            logger.error(f"Transformation Failed Error: {str(e)}")
            return TaskStatus.FAILED, e

        except Exception as e:
            logger.error(f"Unexpected Error: {str(e)}")
            return TaskStatus.FAILED, e