from pyspark.sql import SparkSession
from pyspark.sql.utils import AnalysisException
import json
import logging
import time

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("Dynamic SQL Transformation Pipeline") \
    .getOrCreate()

# Set logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load configuration
def load_sql_config(config_path):
    """
    Loads the SQL transformation rules from a JSON configuration file.
    """
    with open(config_path, "r") as config_file:
        return json.load(config_file)

# Read input data
def read_source_data(spark, input_path):
    """
    Reads the source data into a DataFrame.
    """
    return spark.read.parquet(input_path)

# Apply SQL Transformation
def apply_sql_transformation(source_df, sql_query):
    """
    Applies the provided SQL transformation to the source DataFrame.
    """
    source_df.createOrReplaceTempView("customer_data")
    return spark.sql(sql_query)

# Write output data
def write_output_data(target_df, output_path):
    """
    Writes the transformed DataFrame to a target location in Parquet format.
    """
    target_df.write.mode("overwrite").parquet(output_path)

# Log errors
def log_error(error_description):
    """
    Logs errors to a separate file or table.
    """
    error_timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
    logger.error(f"Error at {error_timestamp}: {error_description}")

# Main pipeline
def transformation_pipeline(config_path, input_path, output_path, error_log_path):
    """
    Main transformation pipeline function.
    """
    try:
        # Load SQL transformation rules
        config = load_sql_config(config_path)
        sql_query = config["sql_query"]

        # Read source data
        source_df = read_source_data(spark, input_path)
        logger.info("Source data loaded successfully.")

        # Apply SQL Transformation
        target_df = apply_sql_transformation(source_df, sql_query)
        logger.info("SQL transformation applied successfully.")

        # Write output data
        write_output_data(target_df, output_path)
        logger.info(f"Transformed data written to {output_path} successfully.")

    except AnalysisException as e:
        log_error(f"SQL AnalysisException: {str(e)}")
    except Exception as e:
        log_error(f"Unexpected error: {str(e)}")

# Example Usage
if __name__ == "__main__":
    CONFIG_PATH = "path/to/config.json"  # Path to SQL config file
    INPUT_PATH = "path/to/input_data"    # Path to input Parquet file
    OUTPUT_PATH = "path/to/output_data"  # Path to save output Parquet file
    ERROR_LOG_PATH = "path/to/error_log" # Path to save error logs

    transformation_pipeline(CONFIG_PATH, INPUT_PATH, OUTPUT_PATH, ERROR_LOG_PATH)