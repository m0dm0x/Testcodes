from pyspark.sql.functions import broadcast
from pyspark.storagelevel import StorageLevel

class TransformationTask(Task):
    def __init__(self, task_config: dict):
        self.task_config = task_config
        self.task_id = task_config.get("taskId")
        self.input_dataframe_ids = []

    def validate_and_load_input(self, snapshots, df_list, continue_on_missing, logger):
        self.input_dataframe_ids = [df_id for df_id in df_list if snapshots.get_snapshot(df_id)] \
            if not continue_on_missing else df_list
        logger.info(f"Validated input DataFrames: {self.input_dataframe_ids}")

    def create_temp_views(self, snapshots, logger):
        for df_id in self.input_dataframe_ids:
            input_df = snapshots.get_snapshot(df_id)
            if input_df:
                logger.info(f"Creating temporary view for DataFrame: {df_id}")
                input_df.createOrReplaceTempView(df_id)

    def optimize_dataframes(self, snapshots, spark, logger):
        for df_id in self.input_dataframe_ids:
            df = snapshots.get_snapshot(df_id)
            if df:
                num_partitions = df.rdd.getNumPartitions()
                logger.info(f"DataFrame '{df_id}' has {num_partitions} partitions.")

                # Default parallelism for optimization hint
                default_parallelism = spark.sparkContext.defaultParallelism

                if num_partitions < default_parallelism:  # Likely small dataset
                    logger.info(f"Broadcasting and caching small DataFrame: {df_id}")
                    df = broadcast(df).persist(StorageLevel.MEMORY_AND_DISK)
                elif num_partitions > 2 * default_parallelism:  # Likely large dataset
                    logger.info(f"Coalescing large DataFrame: {df_id}")
                    df = df.coalesce(default_parallelism)
                else:  # Medium-sized dataset
                    logger.info(f"Repartitioning medium DataFrame: {df_id}")
                    df = df.repartition(default_parallelism)

                # Column pruning to reduce data shuffling
                df = df.select(*df.columns)
                df.cache()
                logger.info(f"Optimizations applied to DataFrame '{df_id}'")

                # Unpersist after use
                df.unpersist(blocking=True)
                logger.info(f"Unpersisted DataFrame '{df_id}'")

    def execute(self):
        status = TaskStatus.SUCCESS, None
        spark = EdhPySparkSingleton.get_instance()
        logger = EdhPySparkLogger.get_instance()
        snapshots = TaskSnapshot.get_instance()

        try:
            input_df_ids = self.task_config.get("inputDataFrame", "").split(self.task_config.get("delimiter", ","))
            output_df = self.task_config.get("outputDataFrame")
            sql_path = self.task_config.get("sqlFile")
            bucket, path = sql_path.split("/", 1)

            sql_query = read_generic_file_from_gcs(bucket, path)

            validate_and_load_input(snapshots, input_df_ids, self.task_config.get("continueOnMissing", False), logger)
            create_temp_views(snapshots, logger)
            
            # Apply DataFrame optimizations
            optimize_dataframes(snapshots, spark, logger)

            logger.info("Executing SQL query...")
            transformed_df = spark.sql(sql_query)
            transformed_df.show()
            snapshots.add_snapshot(output_df, transformed_df)
            logger.info(f"Transformation completed: {output_df}")
        except Exception as e:
            logger.error(f"Transformation failed: {e}")
            status = TaskStatus.FAILURE, str(e)

        return status