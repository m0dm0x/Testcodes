Class: TransformationTask
- Inherits from Task and defines a process to transform data using PySpark and SQL queries.

__init__ (Constructor)
- Initializes the task with a configuration dictionary (`task_config`).
- Extracts the `taskId` for logging and debugging purposes.

Start
  |
  v
Extract Configurations (`execute`)
- Fetch configurations from `task_config`:
  - `inputDataFrame` (comma-separated DataFrame IDs).
  - `outputDataFrame` (output DataFrame ID).
  - `sqlFile` (path to the SQL file containing transformation logic).
  - `delimiter` (default: ",").
  - `continueOnMissing` (default: False).
  - Log these details for reference.
  |
  v
Split Input DataFrame IDs (`execute`)
- Split the `inputDataFrame` string into individual DataFrame IDs using the delimiter.
  |
  v
Check if All Input DataFrames Exist (`execute`)
- Verify if all input DataFrames exist using `snapshots.get_snapshot`.
  - If any DataFrame is missing:
    - Raise a `TransformationFailedError` if `continueOnMissing` is `False`.
    - Log a warning and continue if `continueOnMissing` is `True`.
  |
  +--------------------------------------+
  |                                      |
  v                                      v
All DataFrames Exist?                Missing DataFrame
(`execute`)                          (`execute`)
  |                                      |
  v                                      v
Create Temporary Views               Continue or Fail
and Optimize Input DataFrames        (Based on `continueOnMissing`)
(`execute`)
- For each available DataFrame:
  - Create a temporary SQL view.
  - Apply optimizations based on size:
    - Small (< 1M rows): Use `broadcast` and `persist`.
    - Medium: Repartition for parallelism.
    - Large (> 1B rows): Use `coalesce`.
  - Log logical and physical Spark plans using `input_df.explain`.
  |
  v
Read SQL File (`execute`)
- Read the SQL query from the specified `sqlFile`.
  |
  v
Execute SQL Query (`spark.sql`)
- Execute the SQL query on the input views to produce a transformed DataFrame.
  |
  v
Show Logical and Physical Plans (`transformed_df.explain`)
- Log Spark's logical and physical plans for debugging and performance analysis.
  |
  v
Persist Transformed DataFrame (`transformed_df.persist`)
- Cache the transformed DataFrame using `MEMORY_AND_DISK`.
- Trigger persistence with an action (`transformed_df.count()`).
  |
  v
Add Transformed DataFrame to Snapshots (`snapshots.add_snapshot`)
- Save the transformed DataFrame to the snapshot store for future tasks.
  |
  v
End
  |
  v
Error Handling
  - Catch specific errors:
    - `PySparkRuntimeError`: Log and return `TaskStatus.FAILED`.
    - `TransformationFailedError`: Log and return `TaskStatus.FAILED`.
  - Catch all other exceptions, log them, and return `TaskStatus.FAILED`.