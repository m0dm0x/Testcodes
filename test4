from pyspark.errors import PySparkRuntimeError
from edh_ingestion_modules.models.task import Task
from edh_ingestion_modules.models.task_status import TaskStatus
from edh_ingestion_modules.utilities.service_factory import ServiceFactory
from edh_ingestion_modules.utilities.exceptions import TransformationFailedError

class TransformationTask(Task):
    def __init__(self, config: dict):
        """
        Initialize the TransformationTask with the given configuration.
        """
        self.config = config

    def execute(self):
        """
        Execute the transformation task.
        """
        # Initialize status as SUCCESS
        status = TaskStatus.SUCCESS, None

        # Accessing required instances
        spark = self.spark_instance
        logger = self.logger_instance
        snapshots = self.snapshot_instance
        config = self.config

        logger.info("Starting TransformationTask...")

        try:
            # Fetch input DataFrames
            input_dataframes = config.get("inputDataFrame", "").split(",")
            sql_file_path = config.get("sqlFile")
            output_df_id = config.get("outputDataFrame")
            delimiter = config.get("delimiter", ",")
            continue_on_missing = config.get("continueOnMissing", False)

            # Validate input DataFrames
            logger.info(f"Validating input DataFrames: {input_dataframes}")
            all_exist = all(snapshots.get_snapshot(df) is not None for df in input_dataframes)

            if not all_exist and not continue_on_missing:
                raise TransformationFailedError("One or more input DataFrames are missing.")

            # Register input DataFrames as temporary views
            for df_id in input_dataframes:
                df = snapshots.get_snapshot(df_id)
                if df is not None:
                    df.createOrReplaceTempView(df_id)
                    logger.info(f"Registered DataFrame {df_id} as a temporary view.")
                elif continue_on_missing:
                    logger.warning(f"Input DataFrame {df_id} is missing, but `continueOnMissing` is True.")

            # Execute SQL transformation
            logger.info(f"Executing SQL transformation from file: {sql_file_path}")
            with open(sql_file_path, "r") as sql_file:
                sql_query = sql_file.read()

            transformed_df = spark.sql(sql_query)
            logger.info("SQL transformation executed successfully.")

            # Add the transformed DataFrame to snapshots
            logger.info(f"Storing transformed DataFrame with ID: {output_df_id}")
            snapshots.add_snapshot(output_df_id, transformed_df)

        except TransformationFailedError as e:
            logger.error(f"Transformation failed: {e}")
            status = TaskStatus.FAILED, e

        except PySparkRuntimeError as e:
            logger.error(f"PySpark runtime error during transformation: {e}")
            status = TaskStatus.FAILED, e

        except FileNotFoundError as e:
            logger.error(f"SQL file not found: {e}")
            status = TaskStatus.FAILED, e

        except Exception as e:
            logger.error(f"Unexpected error during transformation: {e}")
            status = TaskStatus.FAILED, e

        return status