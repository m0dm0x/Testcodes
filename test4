from pyspark.sql import SparkSession
from edh_ingestion_modules.models.task import Task
from edh_ingestion_modules.models.task_status import TaskStatus
from edh_ingestion_modules.utilities.exceptions import TransformationFailedError, ServiceNotFoundException
from edh_ingestion_modules.utilities.service_factory import ServiceFactory


class TransformationTask(Task):
    """
    TransformationTask applies SQL transformations to a given input DataFrame
    and stores the result as an output DataFrame in the TaskSnapshot.
    """

    def __init__(self, config: dict):
        """
        Initialize the task with the provided configuration.

        :param config: A dictionary containing task-specific configuration.
        """
        self.config = config

    def execute(self):
        """
        Execute the transformation task.

        - Dynamically resolves a service using ServiceFactory if required.
        - Reads the input DataFrame from the TaskSnapshot.
        - Loads SQL transformation logic (from file or service).
        - Applies the SQL query to the input DataFrame.
        - Stores the transformed DataFrame in the TaskSnapshot.

        :return: Tuple of TaskStatus and optional Exception.
        """
        status = TaskStatus.SUCCESS, None
        spark = self.spark_instance
        logger = self.logger_instance
        snapshots = self.snapshot_instance
        config = self.config

        try:
            logger.info("Starting Transformation Task...")

            # Get input and output DataFrame IDs from the config
            input_df_id = config.get("input_df_id")
            output_df_id = config.get("output_df_id")
            sql_file_path = config.get("sql_file")

            # Validate presence of input DataFrame in snapshot
            input_df = snapshots.get_snapshot(input_df_id)
            if input_df is None:
                raise TransformationFailedError(f"Input DataFrame '{input_df_id}' not found in TaskSnapshot.")

            logger.info(f"Input DataFrame '{input_df_id}' found. Proceeding with transformation.")

            # Dynamically resolve a service if specified in the configuration
            service = None
            if "service_type" in config:
                logger.info("Resolving transformation service using ServiceFactory...")
                service = ServiceFactory.resolve_service(config)
                logger.info(f"Service resolved successfully: {service.__class__.__name__}")

            # Load SQL query (from file or via the resolved service)
            if service:
                logger.info("Loading SQL query using resolved service...")
                sql_query = service.read()  # Assuming service.read() returns a query string
            else:
                logger.info("Loading SQL query from file...")
                with open(sql_file_path, 'r') as sql_file:
                    sql_query = sql_file.read()

            logger.info("SQL query loaded successfully.")

            # Register input DataFrame as a temporary view for SQL execution
            input_df.createOrReplaceTempView(input_df_id)

            # Execute the SQL query
            logger.info(f"Executing SQL query on input DataFrame '{input_df_id}'.")
            transformed_df = spark.sql(sql_query)
            logger.info("SQL query executed successfully.")

            # Add the transformed DataFrame to the snapshot with the output ID
            snapshots.add_snapshot(output_df_id, transformed_df)
            logger.info(f"Transformed DataFrame added to TaskSnapshot with ID '{output_df_id}'.")

        except ServiceNotFoundException as e:
            logger.error(f"Service resolution failed: {str(e)}")
            status = TaskStatus.FAILED, e

        except Exception as e:
            # Catch any transformation errors and update the status
            logger.error(f"Transformation task failed: {str(e)}")
            status = TaskStatus.FAILED, e

        return status

    def is_executable(self) -> bool:
        """
        Determine if the task is executable based on the current status.
        The task will execute if the pipeline has not failed.

        :return: Boolean indicating executability of the task.
        """
        return self.snapshot_instance.status != TaskStatus.FAILED