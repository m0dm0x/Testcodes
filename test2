from edh_ingestion_modules.models.task import Task, TaskStatus
from edh_ingestion_modules.utilities.logging_utility import EdhPySparkLogger
from edh_ingestion_modules.utilities.task_snapshot_utility import TaskSnapshot
from pyspark.sql.functions import (
    col, min, max, mean, stddev, count, approx_count_distinct, length, rand, when
)


class GetMetricsTask(Task):
    """
    GetMetricsTask provides insights for input DataFrames, such as schema,
    row counts, statistics, and Spark execution plans.

    Configuration example:
    {
        "taskId": "get_metrics_task",
        "inputDataFrames": "input_df1,input_df2",
        "display_head": "true",
        "count_rows": "true",
        "print_schema": "true",
        "numeric_stats": "true",
        "string_stats": "true",
        "data_type_summary": "true",
        "sample_data": "true",
        "log_spark_plan": "true"
    }
    """

    def __init__(self, config: dict):
        self.config = config
        self.task_id = config.get("taskId")

    def _display_dataframe(self, df, logger, num_rows=20):
        logger.info("Displaying DataFrame head:")
        df.show(num_rows)

    def _count_rows(self, df, logger):
        row_count = df.count()
        logger.info(f"Total number of rows: {row_count}")

    def _print_schema(self, df, logger):
        logger.info("DataFrame Schema:")
        df.printSchema()

    def _calculate_numeric_stats(self, df, logger):
        numeric_cols = [field.name for field in df.schema.fields if field.dataType.typeName() in ('integer', 'double', 'float', 'long')]
        if numeric_cols:
            logger.info("Numeric Column Statistics:")
            for col_name in numeric_cols:
                stats = df.select(
                    min(col(col_name)).alias(f"min_{col_name}"),
                    max(col(col_name)).alias(f"max_{col_name}"),
                    mean(col(col_name)).alias(f"mean_{col_name}"),
                    stddev(col(col_name)).alias(f"stddev_{col_name}"),
                    count(col(col_name)).alias(f"non_null_{col_name}"),
                    count(when(col(col_name).isNull(), col_name)).alias(f"null_{col_name}")
                ).collect()[0]
                logger.info(f"Stats for {col_name}: {stats}")
        else:
            logger.info("No numeric columns found.")

    def _calculate_string_stats(self, df, logger):
        string_cols = [field.name for field in df.schema.fields if field.dataType.typeName() == 'string']
        if string_cols:
            logger.info("String Column Statistics:")
            for col_name in string_cols:
                stats = df.select(
                    approx_count_distinct(col(col_name)).alias(f"unique_{col_name}"),
                    count(when(col(col_name).isNull(), col(col_name))).alias(f"null_{col_name}"),
                    max(length(col(col_name))).alias(f"max_length_{col_name}")
                ).collect()[0]
                logger.info(f"Stats for {col_name}: {stats}")
        else:
            logger.info("No string columns found.")

    def _data_type_summary(self, df, logger):
        type_counts = {}
        for field in df.schema.fields:
            data_type = field.dataType.typeName()
            type_counts[data_type] = type_counts.get(data_type, 0) + 1
        logger.info("Data Type Summary:")
        for data_type, count in type_counts.items():
            logger.info(f"{data_type}: {count} columns")

    def _output_sample_data(self, df, logger, sample_size=5):
        logger.info("Sample Data:")
        df.orderBy(rand()).limit(sample_size).show()

    def _log_spark_plan(self, df, logger):
        logger.info("Execution Plan (Logical and Physical):")
        df.explain(True)

    def execute(self):
        """
        Execute the task and generate metrics for input DataFrames based on configuration flags.

        Returns:
            Tuple: Task status and message.
        """
        status = TaskStatus.SUCCESS, None

        logger = EdhPySparkLogger.get_instance()
        snapshots = TaskSnapshot.get_instance()

        try:
            dataframe_ids = self.config.get("inputDataFrames", "").split(",")

            for df_id in dataframe_ids:
                df = snapshots.get_snapshot(df_id.strip())
                if not df:
                    logger.warning(f"No DataFrame found for id: {df_id}")
                    continue

                logger.info(f"Processing DataFrame with ID: {df_id}")

                # Execute operations based on task-level flags
                if self.config.get("display_head", "false").lower() == "true":
                    self._display_dataframe(df, logger)

                if self.config.get("count_rows", "false").lower() == "true":
                    self._count_rows(df, logger)

                if self.config.get("print_schema", "false").lower() == "true":
                    self._print_schema(df, logger)

                if self.config.get("numeric_stats", "false").lower() == "true":
                    self._calculate_numeric_stats(df, logger)

                if self.config.get("string_stats", "false").lower() == "true":
                    self._calculate_string_stats(df, logger)

                if self.config.get("data_type_summary", "false").lower() == "true":
                    self._data_type_summary(df, logger)

                if self.config.get("sample_data", "false").lower() == "true":
                    self._output_sample_data(df, logger)

                if self.config.get("log_spark_plan", "false").lower() == "true":
                    self._log_spark_plan(df, logger)

        except Exception as e:
            logger.error(f"Task execution failed with error: {e}")
            status = TaskStatus.FAILED, str(e)

        return status