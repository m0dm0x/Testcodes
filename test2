import json
import yaml
import gcsfs
import time
import logging
from pyspark.sql import SparkSession
from pyspark.sql.utils import AnalysisException


class GCSClientSingleton:
    """
    Singleton class to manage GCS client connection.
    Ensures a single instance of the GCSFileSystem is created.
    """
    _instance = None

    @staticmethod
    def get_instance():
        if GCSClientSingleton._instance is None:
            GCSClientSingleton._instance = gcsfs.GCSFileSystem()
        return GCSClientSingleton._instance


class PySparkTransformationPipeline:
    def __init__(self, config_gcs_path, input_gcs_path, output_gcs_path, error_gcs_path):
        self.spark = SparkSession.builder.appName("Dynamic SQL Transformation Pipeline with GCS").getOrCreate()
        self.config_gcs_path = config_gcs_path
        self.input_gcs_path = input_gcs_path
        self.output_gcs_path = output_gcs_path
        self.error_gcs_path = error_gcs_path
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(level=logging.INFO)
        self.gcs_client = GCSClientSingleton.get_instance()

    def load_sql_config(self):
        """
        Loads the SQL transformation rules from a JSON or YAML configuration file in GCS.
        """
        try:
            with self.gcs_client.open(self.config_gcs_path, "r") as config_file:
                content = config_file.read()
                if self.config_gcs_path.endswith(".json"):
                    return json.loads(content)
                elif self.config_gcs_path.endswith(".yaml") or self.config_gcs_path.endswith(".yml"):
                    return yaml.safe_load(content)
                else:
                    raise ValueError("Unsupported configuration file format. Use JSON or YAML.")
        except Exception as e:
            self.log_error(f"Error loading configuration: {str(e)}")
            raise

    def read_source_data(self):
        """
        Reads the source data from GCS into a DataFrame.
        """
        try:
            df = self.spark.read.parquet(self.input_gcs_path)
            self.logger.info("Source data loaded successfully.")
            return df
        except Exception as e:
            self.log_error(f"Error reading source data: {str(e)}")
            raise

    def apply_sql_transformation(self, source_df, sql_query):
        """
        Applies the provided SQL transformation to the source DataFrame.
        """
        try:
            source_df.createOrReplaceTempView("customer_data")
            target_df = self.spark.sql(sql_query)
            self.logger.info("SQL transformation applied successfully.")
            return target_df
        except AnalysisException as e:
            self.log_error(f"SQL AnalysisException: {str(e)}")
            raise
        except Exception as e:
            self.log_error(f"Unexpected error during SQL transformation: {str(e)}")
            raise

    def write_output_data(self, target_df):
        """
        Writes the transformed DataFrame to GCS in Parquet format.
        """
        try:
            target_df.write.mode("overwrite").parquet(self.output_gcs_path)
            self.logger.info(f"Transformed data written to {self.output_gcs_path} successfully.")
        except Exception as e:
            self.log_error(f"Error writing output data: {str(e)}")
            raise

    def log_error(self, error_description):
        """
        Logs errors to a specified GCS path.
        """
        try:
            error_timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
            error_message = f"Error at {error_timestamp}: {error_description}\n"
            self.spark.createDataFrame([(error_message,)], ["error"]).write.mode("append").text(self.error_gcs_path)
        except Exception as e:
            self.logger.error(f"Error logging the error: {str(e)}")

    def run_pipeline(self):
        """
        Executes the full transformation pipeline.
        """
        try:
            # Load SQL transformation rules
            config = self.load_sql_config()
            sql_query = config.get("sql_query")
            if not sql_query:
                raise ValueError("SQL query not found in the configuration file.")

            # Read source data
            source_df = self.read_source_data()

            # Apply SQL Transformation
            target_df = self.apply_sql_transformation(source_df, sql_query)

            # Write output data
            self.write_output_data(target_df)

        except Exception as e:
            self.log_error(f"Pipeline execution failed: {str(e)}")
            raise


# Example Usage
if __name__ == "__main__":
    CONFIG_GCS_PATH = "gs://your-bucket-name/config/config.json"  # Path to SQL config file in GCS (JSON or YAML)
    INPUT_GCS_PATH = "gs://your-bucket-name/input_data"           # Path to input Parquet file in GCS
    OUTPUT_GCS_PATH = "gs://your-bucket-name/output_data"         # Path to save output Parquet file in GCS
    ERROR_GCS_PATH = "gs://your-bucket-name/error_logs"           # Path to save error logs in GCS

    pipeline = PySparkTransformationPipeline(CONFIG_GCS_PATH, INPUT_GCS_PATH, OUTPUT_GCS_PATH, ERROR_GCS_PATH)
    pipeline.run_pipeline()