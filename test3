from typing import Tuple, Optional, Union
from pyspark.sql import SparkSession, DataFrame
from edh_logger import EdhPysparkLogger  # Assuming EdhPysparkLogger is part of your project
from task_snapshot import TaskSnapshot  # Assuming TaskSnapshot is part of your project
from task_status import TaskStatus  # Enum for task execution status
from task import Task  # Abstract base class for all tasks


class TransformationTask(Task):
    """
    Task for applying SQL transformations on input DataFrames.
    Inherits from the abstract Task class.
    """

    def __init__(self, config: dict):
        """
        Initialize the TransformationTask with the given configuration.

        Args:
            config (dict): Configuration for the task, including paths, SQL files, etc.
        """
        super().__init__(config)
        self.sql_file_path = self.config.get("sqlFile")
        self.delimiter = self.config.get("delimiter", ",")
        self.continue_on_missing = self.config.get("continueOnMissing", False)

    def execute(self) -> Tuple[TaskStatus, Optional[Union[Exception, str]]]:
        """
        Execute the transformation task.

        Returns:
            Tuple[TaskStatus, Optional[Union[Exception, str]]]: Execution status and any error description.
        """
        try:
            # Validate input DataFrames
            input_dataframes = self.config["inputDataFrame"].split(self.delimiter)
            output_dataframe = self.config["outputDataFrame"]
            all_exist = all(
                self.snapshot_instance.get_snapshot(df) is not None for df in input_dataframes
            )

            if not all_exist and not self.continue_on_missing:
                raise ValueError("Some input DataFrames are missing and continueOnMissing is False.")

            # Register input DataFrames as temporary views
            for df_id in input_dataframes:
                df = self.snapshot_instance.require_snapshot(df_id)
                df.createOrReplaceTempView(df_id)

            # Read and execute the SQL transformation
            sql_query = self._read_sql_file(self.sql_file_path)
            transformed_df = self.spark_instance.sql(sql_query)

            # Add transformed DataFrame to the snapshot
            self.snapshot_instance.add_snapshot(output_dataframe, transformed_df)

            self.logger_instance.info(f"TransformationTask completed successfully for {output_dataframe}.")
            return TaskStatus.SUCCESS, None

        except Exception as e:
            error_message = f"Error in TransformationTask: {str(e)}"
            self.logger_instance.error(error_message)
            return TaskStatus.FAILED, error_message

    def is_executable(self) -> bool:
        """
        Determine if the task is executable based on the current context.
        
        Returns:
            bool: True if the task can be executed, False otherwise.
        """
        return self.snapshot_instance.status != TaskStatus.FAILED

    @staticmethod
    def _read_sql_file(file_path: str) -> str:
        """
        Read the SQL query from the specified file.

        Args:
            file_path (str): Path to the SQL file.

        Returns:
            str: SQL query.
        """
        with open(file_path, "r") as file:
            return file.read()