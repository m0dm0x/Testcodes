import json
import logging
from pyspark.sql import SparkSession
from pathlib import Path
from typing import List

# Assuming you have equivalent utility classes and methods like `ExecutionContext` and `Configuration`.
class TransformWithSQLTask:
    """
    A Python implementation of the TransformWithSQL task using PySpark,
    based on inputs provided dynamically through a JSON configuration.
    """

    def __init__(self, task_id: str, config_path: str, context):
        """
        Initialize the TransformWithSQLTask.

        :param task_id: Unique identifier for the task.
        :param config_path: Path to the JSON configuration file.
        :param context: ExecutionContext object to manage execution state and data.
        """
        self.task_id = task_id
        self.config_path = config_path
        self.context = context
        self.spark = context.spark_session
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(level=logging.INFO)

    def load_config(self):
        """
        Load configuration from the JSON file.

        :return: Configuration dictionary.
        """
        try:
            with open(self.config_path, "r") as file:
                config = json.load(file)
            self.logger.info(f"Configuration loaded successfully for Task ID: {self.task_id}")
            return config
        except Exception as e:
            self.logger.error(f"Failed to load configuration: {str(e)}")
            raise

    def process_data_frames(
        self,
        input_data_frames: str,
        output_data_frame: str,
        sql_file_path: str,
        delimiter: str = ",",
        continue_on_missing: bool = True,
    ):
        """
        Process input DataFrames and apply SQL transformations.

        :param input_data_frames: Comma-separated names of input DataFrames.
        :param output_data_frame: Name of the output DataFrame.
        :param sql_file_path: Path to the SQL file containing transformation logic.
        :param delimiter: Delimiter for splitting input DataFrame names.
        :param continue_on_missing: Whether to continue if some input DataFrames are missing.
        """
        try:
            input_data_frames_list = input_data_frames.split(delimiter)
            all_exist = all(
                [self.context.get_data_snapshot(df.strip()) is not None for df in input_data_frames_list]
            )

            if all_exist:
                # Register all input DataFrames as temporary views
                for df_name in input_data_frames_list:
                    df = self.context.require_data_snapshot(df_name.strip())
                    df.createOrReplaceTempView(df_name.strip())

                # Read and apply the SQL file
                sql_query = Path(sql_file_path).read_text()
                result_df = self.spark.sql(sql_query)

                # Save the output DataFrame
                self.context.add_data_snapshot(output_data_frame, result_df)
                self.logger.info(f"Output DataFrame '{output_data_frame}' processed successfully.")

            elif continue_on_missing:
                self.logger.warning(
                    "Some input DataFrames are missing, but proceeding as 'continueOnMissing' is True."
                )
                for df_name in input_data_frames_list:
                    if self.context.get_data_snapshot(df_name.strip()):
                        df = self.context.require_data_snapshot(df_name.strip())
                        df.createOrReplaceTempView(df_name.strip())

                # Apply a default transformation using the first available DataFrame
                sql_query = f"SELECT * FROM {input_data_frames_list[0].strip()}"
                result_df = self.spark.sql(sql_query)
                self.context.add_data_snapshot(output_data_frame, result_df)
                self.logger.info(f"Output DataFrame '{output_data_frame}' processed with partial inputs.")

            else:
                self.logger.error("Missing input DataFrames and 'continueOnMissing' is False. Task aborted.")
                raise ValueError("Input DataFrames are missing and cannot proceed.")

        except Exception as e:
            self.logger.error(f"Error processing DataFrames: {str(e)}")
            raise

    def execute(self):
        """
        Execute the task.
        """
        try:
            # Load configuration
            config = self.load_config()

            # Extract parameters from configuration
            input_data_frames = config["inputDataFrame"]
            output_data_frame = config["outputDataFrame"]
            sql_file_path = config["sqlFile"]
            delimiter = config.get("delimiter", ",")
            continue_on_missing = config.get("continueOnMissing", True)

            # Process the DataFrames
            self.process_data_frames(
                input_data_frames, output_data_frame, sql_file_path, delimiter, continue_on_missing
            )
        except Exception as e:
            self.logger.error(f"Task execution failed for Task ID '{self.task_id}': {str(e)}")
            raise


# Example usage
if __name__ == "__main__":
    # Example ExecutionContext (assuming you have a compatible implementation)
    class ExecutionContext:
        def __init__(self):
            self.spark_session = SparkSession.builder.appName("TransformWithSQLTask").getOrCreate()
            self.data_snapshots = {}

        def get_data_snapshot(self, name: str):
            return self.data_snapshots.get(name)

        def require_data_snapshot(self, name: str):
            if name not in self.data_snapshots:
                raise ValueError(f"Data snapshot '{name}' is required but not found.")
            return self.data_snapshots[name]

        def add_data_snapshot(self, name: str, df):
            self.data_snapshots[name] = df

    # Simulate a running context and example task
    context = ExecutionContext()

    # Configuration file path
    config_path = "path/to/config.json"

    # Initialize and execute the task
    task = TransformWithSQLTask("TransformWithSql", config_path, context)
    task.execute()