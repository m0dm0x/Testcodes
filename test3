from pyspark.sql import DataFrame
from pyspark.sql.types import StructType
from edh_ingestion_modules.utilities.spark_utility import EdhPySparkSingleton
from edh_ingestion_modules.utilities.gcs_utility import GCSClientSingleton


class CSVService:
    def read_csv_from_gcs(self, spark, config, logger, schema: StructType = None) -> DataFrame:
        """Read CSV data from GCS with configurable options."""
        input_path = config.get("input_path")
        if not input_path.startswith("gs://"):
            raise ValueError("Input path must be a valid GCS path (starting with 'gs://')")

        try:
            logger.info(f"Reading CSV from GCS: {input_path}")
            reader = spark.read.format("csv")

            # Set CSV options from config with defaults
            options = {
                "header": config.get("header", "true"),
                "inferSchema": "false" if schema else "true",
                "delimiter": config.get("delimiter", ","),
                "quote": config.get("quote", '"'),
                "escape": config.get("escape", "\\"),
                "nullValue": config.get("nullValue", ""),
                "emptyValue": config.get("emptyValue", ""),
                "comment": config.get("comment", ""),
                "ignoreLeadingWhiteSpace": config.get("ignoreLeadingWhiteSpace", "false"),
                "ignoreTrailingWhiteSpace": config.get("ignoreTrailingWhiteSpace", "false")
            }

            for key, value in options.items():
                reader = reader.option(key, value)

            if schema:
                reader = reader.schema(schema)

            if compression := config.get("compression"):
                reader = reader.option("compression", compression)

            return reader.load(input_path)

        except Exception as e:
            logger.error(f"CSV read failed: {str(e)}", exc_info=True)
            raise RuntimeError(f"CSV read operation failed: {e}") from e

    def write_csv_to_gcs(self, df: DataFrame, config, logger) -> None:
        """Write DataFrame to GCS as CSV with configurable options."""
        output_path = config.get("output_path")
        if not output_path.startswith("gs://"):
            raise ValueError("Output path must be a valid GCS path (starting with 'gs://')")

        try:
            logger.info(f"Writing CSV to GCS: {output_path}")
            writer = df.write.format("csv")

            # Set CSV options from config with defaults
            options = {
                "header": config.get("header", "true"),
                "delimiter": config.get("delimiter", ","),
                "quote": config.get("quote", '"'),
                "escape": config.get("escape", "\\"),
                "nullValue": config.get("nullValue", ""),
                "quoteAll": config.get("quoteAll", "false")
            }

            for key, value in options.items():
                writer = writer.option(key, value)

            if compression := config.get("compression"):
                writer = writer.option("compression", compression)

            # Handle write mode
            mode = config.get("mode", "overwrite")
            valid_modes = ["overwrite", "append", "ignore", "error", "errorifexists"]
            if mode.lower() not in valid_modes:
                raise ValueError(f"Invalid write mode: {mode}. Valid modes: {', '.join(valid_modes)}")

            writer = writer.mode(mode)

            # Handle partitioning
            partition_cols = config.get("partition_by")
            if partition_cols:
                writer = writer.partitionBy(*partition_cols)

            writer.save(output_path)
            logger.info(f"Successfully wrote CSV to {output_path}")

        except Exception as e:
            logger.error(f"CSV write failed: {str(e)}", exc_info=True)
            raise RuntimeError(f"CSV write operation failed: {e}") from e

    def execute(self) -> None:
        """Execute complete read-write workflow."""
        try:
            spark = EdhPySparkSingleton.get_instance()
            config = EdhPySparkSingleton.get_config()
            logger = EdhPySparkSingleton.get_logger()

            logger.info("Starting CSV processing workflow")
            df = self.read_csv_from_gcs(spark, config, logger)
            self.write_csv_to_gcs(df, config, logger)
            logger.info("CSV processing completed successfully")

        except Exception as e:
            logger.error(f"CSV workflow failed: {str(e)}", exc_info=True)
            raise



{
  "input_path": "gs://your-bucket-name/input-data.csv",
  "output_path": "gs://your-bucket-name/output-data",
  "header": "true",
  "delimiter": ",",
  "quote": "\"",
  "escape": "\\",
  "nullValue": "",
  "emptyValue": "",
  "comment": "",
  "ignoreLeadingWhiteSpace": "false",
  "ignoreTrailingWhiteSpace": "false",
  "compression": "gzip",
  "mode": "overwrite",
  "quoteAll": "false",
  "partition_by": ["column1", "column2"]
}
